{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76976830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "from transformers import AutoTokenizer,AutoModelForMaskedLM,BertTokenizer\n",
    "from utilz import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79369f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from utilz import *\n",
    "\n",
    "# ============ 配置 ============\n",
    "FOLD_FILES = [\n",
    "    ('/home/user2_4/code/shujuji_jupyter/fold0_train.csv', 'fold0'),\n",
    "    ('/home/user2_4/code/shujuji_jupyter/fold1_train.csv', 'fold1'),\n",
    "    ('/home/user2_4/code/shujuji_jupyter/fold2_train.csv', 'fold2')\n",
    "]\n",
    "OUTPUT_DIR = './data2025/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "label_dict = {'无聊': 0, '快乐': 1, '感兴趣': 2, '疲倦': 3, '困惑': 4}\n",
    "\n",
    "# ============ 循环处理每个fold ============\n",
    "print(\"=\"*60)\n",
    "print(\"开始处理3个fold...\")\n",
    "\n",
    "all_fold_data = []\n",
    "\n",
    "for fold_file, fold_name in FOLD_FILES:\n",
    "    print(f\"\\n加载 {fold_file}...\")\n",
    "    \n",
    "    ids, paths, nlps, classes, modes = load_data(fold_file)\n",
    "    \n",
    "    print(f\"  样本数: {len(ids)}, train: {modes.count('train')}, test: {modes.count('test')}\")\n",
    "    \n",
    "    # 组织标签\n",
    "    labels = {'train': [], 'test': []}\n",
    "    for classe, mode in zip(classes, modes):\n",
    "        labels[mode].append(label_dict[classe])\n",
    "    \n",
    "    # 保存标签\n",
    "    label_file = os.path.join(OUTPUT_DIR, f'{fold_name}_labels.pkl')\n",
    "    save_features(labels, label_file)\n",
    "    print(f\"  ✓ 标签已保存: {label_file}\")\n",
    "    print(f\"    train={len(labels['train'])}, test={len(labels['test'])}\")\n",
    "    \n",
    "    # 保存到列表中供后续特征提取使用\n",
    "    all_fold_data.append({\n",
    "        'fold_name': fold_name,\n",
    "        'ids': ids,\n",
    "        'paths': paths,\n",
    "        'nlps': nlps,\n",
    "        'classes': classes,\n",
    "        'modes': modes\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ 标签文件生成完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# ============ 收集所有文本用于训练Word2Vec ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"收集所有文本用于训练Word2Vec模型...\")\n",
    "\n",
    "all_vocabs = []\n",
    "max_len = 0\n",
    "\n",
    "for fold_data in all_fold_data:\n",
    "    for s in fold_data['nlps']:\n",
    "        tokens = list(jieba.lcut(s))\n",
    "        all_vocabs.append(tokens)\n",
    "        if len(tokens) > max_len:\n",
    "            max_len = len(tokens)\n",
    "\n",
    "print(f\"总文本数: {len(all_vocabs)}, 最大序列长度: {max_len}\")\n",
    "\n",
    "# ============ 训练Word2Vec模型 ============\n",
    "print(\"\\n训练Word2Vec模型...\")\n",
    "model_own = Word2Vec(sentences=all_vocabs, vector_size=100, sg=1, min_count=1)\n",
    "model_own.save('word2vec_3fold.model')\n",
    "model_own.train(all_vocabs, total_examples=len(all_vocabs), epochs=10)\n",
    "print(\"✓ Word2Vec模型训练完成\")\n",
    "\n",
    "# ============ 加载BERT模型 ============\n",
    "print(\"\\n加载BERT模型...\")\n",
    "model_path = './bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(model_path, output_hidden_states=True)\n",
    "print(\"✓ BERT模型加载完成\")\n",
    "\n",
    "# ============ 循环处理每个fold的特征提取 ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"开始提取特征...\")\n",
    "\n",
    "for fold_data in all_fold_data:\n",
    "    fold_name = fold_data['fold_name']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"处理 {fold_name}...\")\n",
    "    \n",
    "    # ---- Word2Vec 特征提取 ----\n",
    "    texts_embs_w2v = {'train': [], 'test': []}\n",
    "    \n",
    "    for nlp, mode in zip(fold_data['nlps'], fold_data['modes']):\n",
    "        tokens = list(jieba.lcut(nlp))\n",
    "        tmp_embs = []\n",
    "        for w in tokens:\n",
    "            tmp_embs.append(model_own.wv[w])\n",
    "        \n",
    "        if len(tmp_embs) < max_len:\n",
    "            tmp_embs = np.concatenate([tmp_embs, np.zeros((max_len - len(tmp_embs), 100))], axis=0)\n",
    "        \n",
    "        texts_embs_w2v[mode].append(tmp_embs[:max_len])\n",
    "    \n",
    "    # 保存Word2Vec特征\n",
    "    w2v_file = os.path.join(OUTPUT_DIR, f'{fold_name}_textual_wav2vec.pkl')\n",
    "    save_features(texts_embs_w2v, w2v_file)\n",
    "    print(f\"  ✓ Word2Vec特征已保存: {w2v_file}\")\n",
    "    print(f\"    train={len(texts_embs_w2v['train'])}, test={len(texts_embs_w2v['test'])}\")\n",
    "    \n",
    "    # ---- BERT 特征提取 ----\n",
    "    texts_embs_bert = {'train': [], 'test': []}\n",
    "    \n",
    "    for idx, (nlp, mode) in enumerate(zip(fold_data['nlps'], fold_data['modes'])):\n",
    "        marked_text = '[CLS]' + nlp + '[SEP]'\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensor = torch.tensor([segments_ids])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "            tmp_embs = outputs[-1][0].cpu().detach().numpy()[0]  # (N, 768)\n",
    "        \n",
    "        if len(tmp_embs) < max_len:\n",
    "            tmp_embs = np.concatenate([tmp_embs, np.zeros((max_len - len(tmp_embs), 768))], axis=0)\n",
    "        \n",
    "        texts_embs_bert[mode].append(tmp_embs[:max_len])\n",
    "        \n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"    已处理 {idx + 1}/{len(fold_data['nlps'])} 个样本\")\n",
    "    \n",
    "    # 保存BERT特征\n",
    "    bert_file = os.path.join(OUTPUT_DIR, f'{fold_name}_textual_bert.pkl')\n",
    "    save_features(texts_embs_bert, bert_file)\n",
    "    print(f\"  ✓ BERT特征已保存: {bert_file}\")\n",
    "    print(f\"    train={len(texts_embs_bert['train'])} (shape: {np.array(texts_embs_bert['train'][0]).shape})\")\n",
    "    print(f\"    test={len(texts_embs_bert['test'])} (shape: {np.array(texts_embs_bert['test'][0]).shape})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ 所有特征提取完成！\")\n",
    "print(f\"\\n生成的文件列表:\")\n",
    "print(f\"  ./data2025/fold0_labels.pkl\")\n",
    "print(f\"  ./data2025/fold0_textual_wav2vec.pkl\")\n",
    "print(f\"  ./data2025/fold0_textual_bert.pkl\")\n",
    "print(f\"  ./data2025/fold1_labels.pkl\")\n",
    "print(f\"  ./data2025/fold1_textual_wav2vec.pkl\")\n",
    "print(f\"  ./data2025/fold1_textual_bert.pkl\")\n",
    "print(f\"  ./data2025/fold2_labels.pkl\")\n",
    "print(f\"  ./data2025/fold2_textual_wav2vec.pkl\")\n",
    "print(f\"  ./data2025/fold2_textual_bert.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1389c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 百度百科预训练词向量特征提取 ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"加载百度百科预训练词向量...\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "BAIDU_WORD2VEC_PATH = 'sgns.baidubaike.bigram-char'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(BAIDU_WORD2VEC_PATH, binary=False)\n",
    "print(f\"✓ 词向量加载完成，词汇量: {len(word2vec_model.index_to_key)}\")\n",
    "print(f\"✓ 词向量维度: {word2vec_model.vector_size}\")\n",
    "\n",
    "embedding_dim = word2vec_model.vector_size  # 通常是300维\n",
    "\n",
    "# ============ 提取百度百科特征 ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"开始提取百度百科词向量特征...\")\n",
    "\n",
    "for fold_data in all_fold_data:\n",
    "    fold_name = fold_data['fold_name']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"处理 {fold_name}...\")\n",
    "\n",
    "    texts_embs_baidu = {'train': [], 'test': []}\n",
    "\n",
    "    for idx, (nlp, mode) in enumerate(zip(fold_data['nlps'], fold_data['modes'])):\n",
    "        # 分词\n",
    "        tokens = list(jieba.cut(nlp))\n",
    "\n",
    "        # 转换为词向量\n",
    "        tmp_embs = []\n",
    "        for word in tokens:\n",
    "            if word in word2vec_model:\n",
    "                tmp_embs.append(word2vec_model[word])\n",
    "            else:\n",
    "                # 未登录词用零向量表示\n",
    "                tmp_embs.append(np.zeros(embedding_dim))\n",
    "\n",
    "        # 填充或截断到max_len\n",
    "        if len(tmp_embs) < max_len:\n",
    "            padding = np.zeros((max_len - len(tmp_embs), embedding_dim))\n",
    "            tmp_embs = np.concatenate([tmp_embs, padding], axis=0)\n",
    "        else:\n",
    "            tmp_embs = np.array(tmp_embs[:max_len])\n",
    "\n",
    "        texts_embs_baidu[mode].append(tmp_embs)\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  已处理 {idx + 1}/{len(fold_data['nlps'])} 个样本\")\n",
    "\n",
    "    # 保存特征\n",
    "    baidu_file = os.path.join(OUTPUT_DIR, f'{fold_name}_textual_baidu.pkl')\n",
    "    save_features(texts_embs_baidu, baidu_file)\n",
    "    print(f\"  ✓ 百度百科特征已保存: {baidu_file}\")\n",
    "    print(f\"    train={len(texts_embs_baidu['train'])} (shape: {np.array(texts_embs_baidu['train'][0]).shape})\")\n",
    "    print(f\"    test={len(texts_embs_baidu['test'])} (shape: {np.array(texts_embs_baidu['test'][0]).shape})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ 所有百度百科特征提取完成！\")\n",
    "print(f\"\\n生成的文件列表:\")\n",
    "print(f\"  ./data2025/fold0_textual_baidu.pkl\")\n",
    "print(f\"  ./data2025/fold1_textual_baidu.pkl\")\n",
    "print(f\"  ./data2025/fold2_textual_baidu.pkl\")\n",
    "print(f\"\\n特征格式: {{'train': [样本列表], 'test': [样本列表]}}\")\n",
    "print(f\"每个样本的shape: ({max_len}, {embedding_dim})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
